# ü§ñ LLM-Konfiguration f√ºr Agents

## √úbersicht

Jeder Agent hat spezifische Aufgaben und Anforderungen. Daher sollten verschiedene LLMs f√ºr optimale Performance konfiguriert werden.

## Agent-spezifische LLM-Empfehlungen

### 1. ChatAgent üó£Ô∏è

**Aufgaben:**
- Dialoge mit Benutzern
- R√ºckfragen zu fehlenden Informationen
- Kommunikation auf Deutsch
- Kurze, pr√§zise Antworten

**Anforderungen:**
- ‚úÖ Schnelle Antwortzeiten (gute User Experience)
- ‚úÖ Gute Deutschkenntnisse
- ‚úÖ Verst√§ndliche, freundliche Kommunikation
- ‚úÖ Kontextverst√§ndnis f√ºr R√ºckfragen

**Empfohlene LLMs (Priorit√§t):**

1. **Qwen2.5:32B** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Aktuell konfiguriert)
   - Gr√∂√üe: 32B Parameter
   - RAM: ~40GB
   - Geschwindigkeit: ‚ö°‚ö° (schnell f√ºr 32B)
   - Qualit√§t: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (exzellent)
   - **Warum:** Sehr starke Qualit√§t f√ºr Dialoge, ausgezeichnete Deutschkenntnisse, hohe Kontextverarbeitung

**Konfiguration:**
```env
OLLAMA_MODEL_CHAT=Qwen2.5:32B
```

---

### 2. DocumentAgent üìÑ

**Aufgaben:**
- PDF-Analyse und Text-Extraktion
- Dokumenten-Kategorisierung (Hotel, Restaurant, Maut, etc.)
- Strukturierte Daten-Extraktion (Betr√§ge, Daten, Steuernummern)
- Vollst√§ndigkeitspr√ºfung
- Validierung und Qualit√§tspr√ºfung
- JSON-Generierung
- **Optional:** Vision-Analyse f√ºr gescannte Dokumente/Foto-Belege (mit Vision-Modell)

**Anforderungen:**
- ‚úÖ Hohe Pr√§zision bei Datenextraktion
- ‚úÖ Gute Strukturerkennung
- ‚úÖ Zuverl√§ssige JSON-Generierung
- ‚úÖ Mehr Kontext f√ºr komplexe Dokumente
- ‚úÖ Multilingual (internationale Belege)
- ‚úÖ **Optional:** Bildverarbeitung f√ºr gescannte Dokumente/Fotos

**Empfohlene LLMs (Priorit√§t):**

1. **Qwen2.5vl:7b** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Aktuell konfiguriert)
   - Gr√∂√üe: 7B Parameter (mit Vision-F√§higkeiten)
   - RAM: ~10GB
   - Geschwindigkeit: ‚ö°‚ö° (schnell)
   - Qualit√§t: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (exzellent f√ºr Bilder und Dokumente)
   - **Warum:** **Vision-Modell f√ºr gescannte Dokumente und Foto-Belege** - kann Bilder direkt analysieren, sehr gute Datenextraktion aus Dokumenten
   - **Anwendung:** Ideal f√ºr PDFs, gescannte Dokumente und Foto-Belege

**Konfiguration:**
```env
OLLAMA_MODEL_DOCUMENT=Qwen2.5vl:7b
```

**Hinweis:** Qwen2.5vl ist ein Vision-Modell, das sowohl Text als auch Bilder verarbeiten kann. Es ist ideal f√ºr gescannte Dokumente, Foto-Belege und PDFs mit Bildern.

---

### 3. AccountingAgent üí∞

**Aufgaben:**
- Dokument-Zuordnung zu Reiseeintr√§gen
- Kategorisierung (Hotel, Verpflegung, Transport, etc.)
- Verpflegungsmehraufwand-Berechnung
- Machbarkeitspr√ºfung (√ºberlappende Hotels, Datum-Abgleich)
- Logik-Validierung
- Komplexe Entscheidungen
- Mathematische Berechnungen

**Anforderungen:**
- ‚úÖ Sehr gute Logik-F√§higkeiten
- ‚úÖ Mathematische Genauigkeit
- ‚úÖ Komplexe Entscheidungsfindung
- ‚úÖ Kontextverst√§ndnis f√ºr Zuordnungen
- ‚úÖ Konsistenz-Pr√ºfung

**Empfohlene LLMs (Priorit√§t):**

1. **DeepSeek-R1:32B** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Aktuell konfiguriert)
   - Gr√∂√üe: 32B Parameter (Reasoning-Modell)
   - RAM: ~40GB
   - Geschwindigkeit: ‚ö°‚ö° (schnell f√ºr 32B)
   - Qualit√§t: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (exzellent f√ºr Logik und Reasoning)
   - **Warum:** **Reasoning-Modell mit hervorragenden Logik-F√§higkeiten** - ideal f√ºr komplexe Entscheidungen, mathematische Berechnungen, Zuordnungen und Machbarkeitspr√ºfungen
   - **Anwendung:** Perfekt f√ºr buchhaltungsrelevante Aufgaben mit hohen Anforderungen an Logik und Genauigkeit

**Konfiguration:**
```env
OLLAMA_MODEL_ACCOUNTING=DeepSeek-R1:32B
```

**Hinweis:** DeepSeek-R1 ist speziell f√ºr Reasoning-Aufgaben optimiert und bietet ausgezeichnete Logik-F√§higkeiten f√ºr komplexe Buchhaltungsaufgaben.

---

## Konfiguration

### 1. Environment-Variablen (.env Datei)

Erstellen Sie eine `.env` Datei im `backend/` Verzeichnis:

```env
# Ollama Server (GMKTec evo x2)
OLLAMA_BASE_URL=http://192.168.178.155:11434

# Standard-Modell (Fallback)
OLLAMA_MODEL=Qwen2.5:32B

# Agent-spezifische Modelle
OLLAMA_MODEL_CHAT=Qwen2.5:32B
OLLAMA_MODEL_DOCUMENT=Qwen2.5vl:7b
OLLAMA_MODEL_ACCOUNTING=DeepSeek-R1:32B

# Timeout-Konfiguration (l√§nger f√ºr gro√üe Modelle)
OLLAMA_TIMEOUT=600
OLLAMA_MAX_RETRIES=3
OLLAMA_RETRY_DELAY=2.0
```

### 2. Docker Compose (docker-compose.agents.yml)

```yaml
environment:
  # Ollama Server
  - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://192.168.178.155:11434}
  
  # Agent-spezifische Modelle
  - OLLAMA_MODEL=${OLLAMA_MODEL:-Qwen2.5:32B}
  - OLLAMA_MODEL_CHAT=${OLLAMA_MODEL_CHAT:-Qwen2.5:32B}
  - OLLAMA_MODEL_DOCUMENT=${OLLAMA_MODEL_DOCUMENT:-Qwen2.5vl:7b}
  - OLLAMA_MODEL_ACCOUNTING=${OLLAMA_MODEL_ACCOUNTING:-DeepSeek-R1:32B}
  
  # Timeout-Konfiguration (l√§nger f√ºr gro√üe Modelle)
  - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-600}
  - OLLAMA_MAX_RETRIES=${OLLAMA_MAX_RETRIES:-3}
  - OLLAMA_RETRY_DELAY=${OLLAMA_RETRY_DELAY:-2.0}
```

### 3. Direkt in agents.py

Die Konfiguration erfolgt √ºber Environment-Variablen (Zeilen 42-46):

```python
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'Qwen2.5:32B')
OLLAMA_MODEL_CHAT = os.getenv('OLLAMA_MODEL_CHAT', OLLAMA_MODEL)
OLLAMA_MODEL_DOCUMENT = os.getenv('OLLAMA_MODEL_DOCUMENT', OLLAMA_MODEL)
OLLAMA_MODEL_ACCOUNTING = os.getenv('OLLAMA_MODEL_ACCOUNTING', OLLAMA_MODEL)
```

**Hinweis:** √Ñndern Sie nicht direkt `agents.py`, sondern verwenden Sie Environment-Variablen!

---

## Modell-Download auf GMKTec evo x2

Laden Sie die empfohlenen Modelle auf Ihrem GMKTec-Server herunter:

```bash
# Auf GMKTec evo x2 Rechner

# ChatAgent (32B Modell - hohe Qualit√§t)
ollama pull Qwen2.5:32B

# DocumentAgent (Vision-Modell - f√ºr Dokumente und Bilder)
ollama pull Qwen2.5vl:7b

# AccountingAgent (Reasoning-Modell - stark bei Logik)
ollama pull DeepSeek-R1:32B
```

**Wichtig:** Diese Modelle ben√∂tigen ausreichend RAM:
- Qwen2.5:32B: ~40GB RAM
- Qwen2.5vl:7b: ~10GB RAM
- DeepSeek-R1:32B: ~40GB RAM

**Gesamt:** Empfohlen mindestens 64GB+ RAM f√ºr gleichzeitige Nutzung

**Pr√ºfen Sie verf√ºgbare Modelle:**
```bash
ollama list
```

---

## Empfohlene Konfigurationen

### Aktuelle Konfiguration (64GB+ RAM empfohlen) ‚≠ê **AKTUELL VERWENDET**

```env
OLLAMA_MODEL_CHAT=Qwen2.5:32B           # ~40GB RAM
OLLAMA_MODEL_DOCUMENT=Qwen2.5vl:7b      # ~10GB RAM
OLLAMA_MODEL_ACCOUNTING=DeepSeek-R1:32B # ~40GB RAM
```
**Gesamt:** ~50GB RAM (wenn Modelle nicht gleichzeitig geladen sind), bis zu ~90GB bei gleichzeitiger Nutzung

**Vorteile:**
- Sehr hohe Qualit√§t f√ºr alle Agenten
- Vision-Unterst√ºtzung f√ºr Dokumente und Bilder
- Exzellente Reasoning-F√§higkeiten f√ºr Buchhaltung
- Beste Ergebnisse bei komplexen F√§llen

**Nachteile:**
- Hohe Ressourcen-Anforderungen (mindestens 64GB RAM empfohlen)
- Langsamere Antwortzeiten als kleinere Modelle
- Ben√∂tigt leistungsstarke Hardware

**Hinweis:** Die aktuellen Modelle sind f√ºr maximale Qualit√§t optimiert. F√ºr ressourcen-effizientere Alternativen siehe Abschnitt "Empfohlene LLMs" bei jedem Agent.

---

## Performance-Optimierung

### Timeout-Konfiguration

Gr√∂√üere Modelle (32B Parameter) ben√∂tigen deutlich mehr Zeit:

```env
# F√ºr gro√üe Modelle (32B Parameter - Standard f√ºr aktuelle Konfiguration)
OLLAMA_TIMEOUT=600

# F√ºr sehr gro√üe Modelle oder komplexe Anfragen
OLLAMA_TIMEOUT=900

# F√ºr kleinere Modelle (falls gewechselt wird)
# OLLAMA_TIMEOUT=300
```

### Retry-Konfiguration

F√ºr stabileres Netzwerk:

```env
OLLAMA_MAX_RETRIES=5
OLLAMA_RETRY_DELAY=3.0
```

---

## Testen der Konfiguration

### 1. Health Check

```bash
# Von Proxmox-Server
curl http://192.168.178.155:11434/api/tags
```

### 2. Test einzelner Agents

```python
from agents import OllamaLLM

# ChatAgent
chat_llm = OllamaLLM(base_url="http://192.168.178.155:11434", model="Qwen2.5:32B")
response = await chat_llm.chat([{"role": "user", "content": "Hallo!"}])
print(f"ChatAgent: {response[:100]}")

# DocumentAgent
doc_llm = OllamaLLM(base_url="http://192.168.178.155:11434", model="Qwen2.5vl:7b")
response = await doc_llm.chat([{"role": "user", "content": "Analysiere Dokument..."}])
print(f"DocumentAgent: {response[:100]}")

# AccountingAgent
acc_llm = OllamaLLM(base_url="http://192.168.178.155:11434", model="DeepSeek-R1:32B")
response = await acc_llm.chat([{"role": "user", "content": "Ordne Dokument zu..."}])
print(f"AccountingAgent: {response[:100]}")
```

### 3. Logs √ºberpr√ºfen

```bash
# Im Agent-Container
docker logs stundenzettel-agents | grep "Ollama LLM erreichbar"
```

Sie sollten sehen:
```
‚úÖ Ollama LLM erreichbar f√ºr ChatAgent: http://192.168.178.155:11434 (Modell: Qwen2.5:32B)
‚úÖ Ollama LLM erreichbar f√ºr DocumentAgent: http://192.168.178.155:11434 (Modell: Qwen2.5vl:7b)
‚úÖ Ollama LLM erreichbar f√ºr AccountingAgent: http://192.168.178.155:11434 (Modell: DeepSeek-R1:32B)
```

---

## Zusammenfassung

| Agent | Aktuelles Modell | RAM | Priorit√§t |
|-------|-------------------|-----|-----------|
| **ChatAgent** | Qwen2.5:32B | ~40GB | Hohe Qualit√§t f√ºr Dialoge |
| **DocumentAgent** | Qwen2.5vl:7b | ~10GB | **Vision-Modell f√ºr Dokumente und Bilder** |
| **AccountingAgent** | DeepSeek-R1:32B | ~40GB | **Reasoning-Modell f√ºr komplexe Logik** |

**Aktuelle Konfiguration:**
```env
OLLAMA_MODEL_CHAT=Qwen2.5:32B
OLLAMA_MODEL_DOCUMENT=Qwen2.5vl:7b
OLLAMA_MODEL_ACCOUNTING=DeepSeek-R1:32B
```

**Hinweis:** Diese Konfiguration ist f√ºr maximale Qualit√§t optimiert und ben√∂tigt mindestens 64GB+ RAM.

**Speicherort:**
- `.env` Datei im `backend/` Verzeichnis
- Oder Environment-Variablen in `docker-compose.agents.yml`

---

**Hinweis:** Starten Sie den Agent-Container nach √Ñnderungen neu:
```bash
docker compose -f docker-compose.agents.yml restart
```
