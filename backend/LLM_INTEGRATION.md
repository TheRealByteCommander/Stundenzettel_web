# ğŸ¤– LLM-Integration fÃ¼r Proxmox Deployment

## Ãœbersicht

Die Agenten laufen auf einem **Proxmox-Server** und kommunizieren Ã¼ber das lokale Netzwerk mit den LLMs, die auf einem **GMKTec evo x2** Rechner laufen.

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Proxmox Server    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Agent Containerâ”‚  â”‚
â”‚  â”‚  (agents.py)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTP/API
           â”‚ (Port 11434)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GMKTec evo x2      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Ollama Server â”‚  â”‚
â”‚  â”‚  (localhost:  â”‚  â”‚
â”‚  â”‚   11434)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Konfiguration

### 1. Ollama auf GMKTec evo x2 installieren

```bash
# Auf GMKTec evo x2 Rechner
curl -fsSL https://ollama.ai/install.sh | sh

# Ollama starten
ollama serve

# Modell herunterladen (z.B. llama3.2)
ollama pull llama3.2

# Alternative: GrÃ¶ÃŸere Modelle fÃ¼r bessere QualitÃ¤t
ollama pull llama3.1:8b
ollama pull mistral:7b
```

### 2. Netzwerk-Zugriff konfigurieren

#### Auf GMKTec evo x2 (Ollama-Server)

StandardmÃ¤ÃŸig hÃ¶rt Ollama nur auf `localhost`. FÃ¼r Netzwerk-Zugriff:

**Option A: Ollama standardmÃ¤ÃŸig auf allen Interfaces starten**

Ollama hÃ¶rt standardmÃ¤ÃŸig auf `0.0.0.0:11434`, also sollten Sie von anderen Rechnern im Netzwerk zugreifen kÃ¶nnen.

**Option B: Firewall-Regel erstellen (falls notwendig)**

```bash
# UFW (Ubuntu/Debian)
sudo ufw allow from 192.168.1.0/24 to any port 11434

# Firewalld (CentOS/RHEL)
sudo firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.1.0/24" port protocol="tcp" port="11434" accept'
sudo firewall-cmd --reload
```

### 3. IP-Adresse des GMKTec-Servers ermitteln

```bash
# Auf GMKTec evo x2
ip addr show
# oder
hostname -I
```

Notieren Sie sich die IP-Adresse (z.B. `192.168.1.100`).

### 4. Environment-Variablen konfigurieren

#### FÃ¼r direkte Python-AusfÃ¼hrung

Erstellen Sie `.env` Datei in `backend/`:

```bash
# .env
OLLAMA_BASE_URL=http://192.168.1.100:11434
OLLAMA_MODEL=llama3.2
OLLAMA_TIMEOUT=300
OLLAMA_MAX_RETRIES=3
OLLAMA_RETRY_DELAY=2.0
```

#### FÃ¼r Docker/Proxmox Container

In `docker-compose.agents.yml` oder `.env`:

```yaml
environment:
  - OLLAMA_BASE_URL=http://192.168.1.100:11434
  - OLLAMA_MODEL=llama3.2
```

## Proxmox Deployment

### Option 1: LXC Container

1. **Container erstellen:**
   ```bash
   # Im Proxmox Web-Interface oder CLI
   pct create 100 ubuntu-22.04-standard \
     --hostname agents \
     --memory 2048 \
     --cores 2 \
     --net0 name=eth0,bridge=vmbr0,ip=dhcp
   ```

2. **Container starten:**
   ```bash
   pct start 100
   pct enter 100
   ```

3. **Python & Dependencies installieren:**
   ```bash
   apt update
   apt install python3.11 python3-pip python3-venv
   ```

4. **Application installieren:**
   ```bash
   cd /opt
   git clone <repository>
   cd stundenzettel_web/backend
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

5. **Systemd Service erstellen:**
   ```bash
   # /etc/systemd/system/agents.service
   [Unit]
   Description=Stundenzettel Agents
   After=network.target

   [Service]
   Type=simple
   User=agentuser
   WorkingDirectory=/opt/stundenzettel_web/backend
   Environment="PATH=/opt/stundenzettel_web/backend/venv/bin"
   EnvironmentFile=/opt/stundenzettel_web/backend/.env
   ExecStart=/opt/stundenzettel_web/backend/venv/bin/python3 -c "from agents import AgentOrchestrator; import asyncio; asyncio.run(...)"
   Restart=always

   [Install]
   WantedBy=multi-user.target
   ```

### Option 2: Docker Container in Proxmox VM

1. **VM erstellen** (Ubuntu/Debian Server)

2. **Docker installieren:**
   ```bash
   curl -fsSL https://get.docker.com -o get-docker.sh
   sh get-docker.sh
   ```

3. **Docker Compose installieren:**
   ```bash
   apt install docker-compose-plugin
   ```

4. **Container starten:**
   ```bash
   cd /opt/stundenzettel_web/backend
   docker compose -f docker-compose.agents.yml up -d
   ```

## Verbindung testen

### 1. Von Proxmox-Server zu GMKTec-Server

```bash
# Von Proxmox (Agent-Server)
curl http://192.168.1.100:11434/api/tags

# Sollte JSON mit verfÃ¼gbaren Modellen zurÃ¼ckgeben:
# {"models": [{"name": "llama3.2", ...}]}
```

### 2. Health Check im Code

```python
from agents import OllamaLLM

llm = OllamaLLM(base_url="http://192.168.1.100:11434")
is_healthy = await llm.health_check()
print(f"Ollama erreichbar: {is_healthy}")
```

### 3. Test-Chat

```python
from agents import OllamaLLM

llm = OllamaLLM(base_url="http://192.168.1.100:11434", model="llama3.2")
response = await llm.chat([
    {"role": "user", "content": "Hallo, kannst du mich hÃ¶ren?"}
])
print(response)
```

## Netzwerk-Troubleshooting

### Problem: "Connection refused"

**LÃ¶sung:**
1. ÃœberprÃ¼fen Sie, ob Ollama auf GMKTec lÃ¤uft:
   ```bash
   # Auf GMKTec
   systemctl status ollama
   # oder
   ps aux | grep ollama
   ```

2. Testen Sie lokalen Zugriff:
   ```bash
   # Auf GMKTec
   curl http://localhost:11434/api/tags
   ```

3. ÃœberprÃ¼fen Sie Firewall:
   ```bash
   # Auf GMKTec
   sudo ufw status
   sudo iptables -L -n | grep 11434
   ```

### Problem: "Timeout"

**LÃ¶sung:**
1. ErhÃ¶hen Sie `OLLAMA_TIMEOUT` in `.env`
2. ÃœberprÃ¼fen Sie Netzwerk-Latenz:
   ```bash
   ping 192.168.1.100
   ```

### Problem: "Model not found"

**LÃ¶sung:**
1. ÃœberprÃ¼fen Sie verfÃ¼gbare Modelle:
   ```bash
   # Auf GMKTec
   ollama list
   ```

2. Passen Sie `OLLAMA_MODEL` in `.env` an

## Performance-Optimierung

### 1. Connection Pooling

Die `OllamaLLM` Klasse verwendet bereits Connection Pooling fÃ¼r bessere Performance bei mehreren Anfragen.

### 2. Timeout-Konfiguration

```bash
# FÃ¼r schnelle Antworten (kleinere Modelle)
OLLAMA_TIMEOUT=60

# FÃ¼r komplexe Analysen (grÃ¶ÃŸere Modelle)
OLLAMA_TIMEOUT=600
```

### 3. Retry-Logik

```bash
# Mehr Retries bei instabilem Netzwerk
OLLAMA_MAX_RETRIES=5
OLLAMA_RETRY_DELAY=3.0
```

## Sicherheit

### 1. Firewall-Regeln

BeschrÃ¤nken Sie Zugriff auf Ollama nur auf Proxmox-Server:

```bash
# Auf GMKTec (UFW)
sudo ufw allow from 192.168.1.50 to any port 11434  # Proxmox IP
sudo ufw deny 11434  # Alle anderen blockieren
```

### 2. VPN (Optional)

FÃ¼r zusÃ¤tzliche Sicherheit kÃ¶nnen Sie ein VPN zwischen Proxmox und GMKTec einrichten.

## Monitoring

### 1. Logs Ã¼berwachen

```bash
# Agent-Logs
tail -f /var/log/agents.log

# Ollama-Logs (auf GMKTec)
journalctl -u ollama -f
```

### 2. Health Checks

```bash
# RegelmÃ¤ÃŸiger Health Check
*/5 * * * * curl -f http://192.168.1.100:11434/api/tags || echo "Ollama down" | mail -s "Alert" admin@example.com
```

## Modell-Empfehlungen

| Modell | GrÃ¶ÃŸe | RAM | Geschwindigkeit | QualitÃ¤t |
|--------|-------|-----|-----------------|----------|
| llama3.2 | 2B | ~4GB | âš¡âš¡âš¡ | â­â­ |
| llama3.1:8b | 8B | ~10GB | âš¡âš¡ | â­â­â­ |
| mistral:7b | 7B | ~10GB | âš¡âš¡ | â­â­â­â­ |
| llama3:70b | 70B | ~80GB | âš¡ | â­â­â­â­â­ |

**Empfehlung:** `llama3.1:8b` oder `mistral:7b` fÃ¼r gute Balance zwischen QualitÃ¤t und Performance.

## NÃ¤chste Schritte

1. âœ… Ollama auf GMKTec installieren
2. âœ… Netzwerk-Zugriff testen
3. âœ… Environment-Variablen konfigurieren
4. âœ… Agents auf Proxmox deployen
5. âœ… Health Check einrichten
6. âœ… Monitoring konfigurieren

---

**LLM-Integration erfolgreich konfiguriert! ğŸ‰**

